{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Attention Seeker ‚Äî Full ML Pipeline with Train/Test Split\n",
        "\n",
        "This notebook processes the wearable sensor dataset, computes the Attention Score,\n",
        "simulates Outside Factors, and **exports train/test CSV files** for model training.\n",
        "\n",
        "Includes:\n",
        "- Preprocessing raw sensor data\n",
        "- 30‚Äësecond windowing\n",
        "- Attention Score computation\n",
        "- Outside Factors simulation\n",
        "- Train/Test splitting\n",
        "- Saving `train.csv` and `test.csv`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "14af8f41",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# ==== CONFIGURE PATHS HERE ====\n",
        "DATA_PATH = \"data/CogLoad1/train/raw/merged_sensors.csv\"   # <-- UPDATE THIS\n",
        "OUTPUT_DIR = \"./attention_output\"          # saves train/test/results\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# Column mappings\n",
        "TIME_COL = \"timestamp\"\n",
        "HR_COL = \"hr\"\n",
        "ACC_X_COL = \"acc_x\"\n",
        "ACC_Y_COL = \"acc_y\"\n",
        "ACC_Z_COL = \"acc_z\"\n",
        "LEVEL_COL = \"level\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "f2916f90",
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_data(path):\n",
        "    df = pd.read_csv(path)\n",
        "    if TIME_COL not in df.columns:\n",
        "        for alt in [\"time\", \"Time\", \"Timestamp\"]:\n",
        "            if alt in df.columns:\n",
        "                df.rename(columns={alt: TIME_COL}, inplace=True)\n",
        "    df[TIME_COL] = pd.to_datetime(df[TIME_COL], errors='coerce')\n",
        "    df = df.dropna(subset=[TIME_COL])\n",
        "    return df\n",
        "\n",
        "def preprocess(df):\n",
        "    df = df.sort_values(TIME_COL)\n",
        "    df['movement'] = np.sqrt(df[ACC_X_COL]**2 + df[ACC_Y_COL]**2 + df[ACC_Z_COL]**2)\n",
        "    df['hr_diff'] = df[HR_COL].diff().abs()\n",
        "    return df.dropna(subset=[HR_COL, 'movement', 'hr_diff'])\n",
        "\n",
        "def compute_baselines(df):\n",
        "    eps = 1e-6\n",
        "    return (\n",
        "        max(df[HR_COL].median(), eps),\n",
        "        max(df['hr_diff'].median(), eps),\n",
        "        max(df['movement'].median(), eps)\n",
        "    )\n",
        "\n",
        "def window_data(df, window_seconds=30):\n",
        "    t0 = df[TIME_COL].min()\n",
        "    df['window_id'] = ((df[TIME_COL] - t0).dt.total_seconds() // window_seconds).astype(int)\n",
        "    agg = {\n",
        "        HR_COL: 'mean',\n",
        "        'hr_diff': 'mean',\n",
        "        'movement': 'mean'\n",
        "    }\n",
        "    if LEVEL_COL in df.columns:\n",
        "        agg[LEVEL_COL] = 'median'\n",
        "    return df.groupby('window_id').agg(agg).reset_index()\n",
        "\n",
        "def compute_attention(df, HR_rest, HRV_rest, M_rest):\n",
        "    eps = 1e-6\n",
        "    df['HR_term'] = (df[HR_COL] - HR_rest) / HR_rest\n",
        "    df['HRV_term'] = (df['hr_diff'] - HRV_rest) / HRV_rest\n",
        "    df['M_term'] = (M_rest - df['movement']) / M_rest\n",
        "    df['AttentionScore'] = 0.25*df['HR_term'] + 0.50*df['HRV_term'] + 0.25*df['M_term']\n",
        "    return df\n",
        "\n",
        "def simulate_outside_factors(df):\n",
        "    rng = np.random.default_rng(42)\n",
        "    n = len(df)\n",
        "    sleep = rng.normal(7.5, 0.7, n)\n",
        "    screen = rng.normal(3.5, 1.0, n)\n",
        "    screen = np.clip(screen, 0.5, 8)\n",
        "    df['Sleep'] = sleep\n",
        "    df['Screen'] = screen\n",
        "    df['OutsideFactors'] = (sleep - 7.5) - (screen - 3.5)\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "504c2fa4",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Columns found: ['datetime', 'timestamp', 'user_id', 'level', 'task', 'hr', 'gsr', 'rr', 'temperature', 'TLX_mean', 'TLX_mental_demand', 'TLX_physical_demand', 'TLX_temporal_demand', 'TLX_performance', 'TLX_effort', 'TLX_frustration', 'acc_x', 'acc_y', 'acc_z', 'opacity_median', 'opacity_std', 'dataset', 'movement', 'hr_diff']\n",
            "                  datetime                     timestamp user_id level   task  \\\n",
            "53092  2017-08-21 11:10:37 1970-01-01 00:25:03.313836500   iz2ps     0  quest   \n",
            "53093  2017-08-21 11:10:38 1970-01-01 00:25:03.313837501   iz2ps     0  quest   \n",
            "53094  2017-08-21 11:10:39 1970-01-01 00:25:03.313838501   iz2ps     0  quest   \n",
            "53095  2017-08-21 11:10:40 1970-01-01 00:25:03.313839488   iz2ps     0  quest   \n",
            "53096  2017-08-21 11:10:41 1970-01-01 00:25:03.313840485   iz2ps     0  quest   \n",
            "\n",
            "         hr       gsr        rr  temperature  TLX_mean  ...  TLX_effort  \\\n",
            "53092  70.0  0.035826  0.326309    29.120001        -1  ...          -1   \n",
            "53093  70.0  0.034954  0.613904    29.120001        -1  ...          -1   \n",
            "53094  70.0  0.034345  0.741109    29.120001        -1  ...          -1   \n",
            "53095  70.0  0.034782  0.846192    29.120001        -1  ...          -1   \n",
            "53096  70.0  0.035742  0.901499    29.120001        -1  ...          -1   \n",
            "\n",
            "       TLX_frustration     acc_x     acc_y     acc_z  opacity_median  \\\n",
            "53092               -1 -0.091878  0.999064  0.033854            -1.0   \n",
            "53093               -1 -0.092489  0.997192  0.034424            -1.0   \n",
            "53094               -1 -0.099976  0.996256  0.026571            -1.0   \n",
            "53095               -1 -0.110962  0.995565  0.030233            -1.0   \n",
            "53096               -1 -0.118937  0.996582  0.031209            -1.0   \n",
            "\n",
            "       opacity_std    dataset  movement  hr_diff  \n",
            "53092         -1.0  iotdata23  1.003851      0.0  \n",
            "53093         -1.0  iotdata23  1.002063      0.0  \n",
            "53094         -1.0  iotdata23  1.001613      0.0  \n",
            "53095         -1.0  iotdata23  1.002186      0.0  \n",
            "53096         -1.0  iotdata23  1.004139      0.0  \n",
            "\n",
            "[5 rows x 24 columns]\n",
            "Converting hr to numeric...\n",
            "Converting hr_diff to numeric...\n",
            "Converting movement to numeric...\n",
            "Converting level to numeric...\n",
            "Dropped 0 rows containing non-numeric data.\n"
          ]
        }
      ],
      "source": [
        "# Standardize column names to what the pipeline expects\n",
        "df = load_data(DATA_PATH)\n",
        "df = df.rename(columns={\n",
        "    'band_ax': 'acc_x',\n",
        "    'band_ay': 'acc_y',\n",
        "    'band_az': 'acc_z',\n",
        "    # Just in case your pipeline expects 'eda' instead of 'gsr' later:\n",
        "    # 'gsr': 'eda' \n",
        "})\n",
        "\n",
        "# Now run the pipeline\n",
        "df = preprocess(df)\n",
        "\n",
        "# Check what the columns are actually named\n",
        "print(\"Columns found:\", df.columns.tolist())\n",
        "print(df.head())\n",
        "\n",
        "# --- INSERT THIS BEFORE RUNNING THE PIPELINE ---\n",
        "\n",
        "# 1. Force critical columns to numeric\n",
        "# 'errors=coerce' turns text like \"Level 1\" or \"Start\" into NaN\n",
        "cols_to_fix = ['hr', 'hr_diff', 'movement']\n",
        "\n",
        "# Only add 'level' if it actually exists in your dataframe\n",
        "if 'level' in df.columns:\n",
        "    cols_to_fix.append('level')\n",
        "\n",
        "for col in cols_to_fix:\n",
        "    if col in df.columns:\n",
        "        print(f\"Converting {col} to numeric...\")\n",
        "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "# 2. Drop any rows where 'hr' or 'movement' became NaN (totally unusable data)\n",
        "before = len(df)\n",
        "df = df.dropna(subset=['hr', 'movement'])\n",
        "print(f\"Dropped {before - len(df)} rows containing non-numeric data.\")\n",
        "\n",
        "# --- NOW RE-RUN YOUR PIPELINE ---\n",
        "# df_w = window_data(df)\n",
        "# ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "6d68c034",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting pipeline with 94331 rows...\n",
            "Baselines -> HR: 73.7, HRV: 0.3, Movement: 1.0\n",
            "Processed windows shape: (1, 12)\n",
            "Saved attention scores to: ./attention_output/attention_scores.csv\n"
          ]
        }
      ],
      "source": [
        "# ==== MODIFIED PIPELINE FOR YOUR LOADED DATA ====\n",
        "\n",
        "# 1. Make sure output directory exists\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# 2. Use your EXISTING df (from previous steps)\n",
        "# We skip 'load_data(DATA_PATH)' because you already have the data in memory\n",
        "print(f\"Starting pipeline with {len(df)} rows...\")\n",
        "\n",
        "# 3. Run the processing steps\n",
        "# Note: We assume you already renamed 'band_ax' -> 'acc_x' in the previous step\n",
        "# If not, uncomment the line below:\n",
        "# df = df.rename(columns={'band_ax': 'acc_x', 'band_ay': 'acc_y', 'band_az': 'acc_z', 'gsr': 'eda'})\n",
        "\n",
        "processed_df = preprocess(df)\n",
        "\n",
        "# 4. Compute Baselines (Resting Heart Rate, etc.)\n",
        "HR_rest, HRV_rest, M_rest = compute_baselines(processed_df)\n",
        "print(f\"Baselines -> HR: {HR_rest:.1f}, HRV: {HRV_rest:.1f}, Movement: {M_rest:.1f}\")\n",
        "\n",
        "# 5. Window the data (30-second chunks)\n",
        "df_w = window_data(processed_df)\n",
        "\n",
        "# 6. Compute Attention Score & Simulate factors\n",
        "df_w = compute_attention(df_w, HR_rest, HRV_rest, M_rest)\n",
        "df_w = simulate_outside_factors(df_w)\n",
        "\n",
        "print(\"Processed windows shape:\", df_w.shape)\n",
        "\n",
        "# 7. Save the intermediate file\n",
        "processed_path = os.path.join(OUTPUT_DIR, \"attention_scores.csv\")\n",
        "df_w.to_csv(processed_path, index=False)\n",
        "print(f\"Saved attention scores to: {processed_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "4c1278ec",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Timestamp fixed! Range: 2017-08-21 11:10:36.500000 to 2017-10-20 12:33:31.401000\n",
            "Windows created: 9494\n",
            "Rows available for training: 9494\n",
            "\n",
            "üéâ SUCCESS! Data is split and saved.\n",
            "Train path: ./attention_output/train.csv\n",
            "Test path: ./attention_output/test.csv\n"
          ]
        }
      ],
      "source": [
        "# ==== FIXING THE TIMESTAMP (CORRECTED UNIT) ====\n",
        "\n",
        "# 1. Reset the timestamp column\n",
        "df['timestamp'] = pd.to_numeric(df['timestamp'])\n",
        "\n",
        "# 2. Convert using 'ms' (Milliseconds) instead of 's'\n",
        "df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')\n",
        "\n",
        "print(f\"‚úÖ Timestamp fixed! Range: {df['timestamp'].min()} to {df['timestamp'].max()}\")\n",
        "\n",
        "# 3. Re-run Windowing (now that time is correct)\n",
        "# Using 10-second windows\n",
        "df_w = window_data(df, window_seconds=10)\n",
        "print(f\"Windows created: {len(df_w)}\")\n",
        "\n",
        "# 4. Resume Pipeline\n",
        "df_w = compute_attention(df_w, HR_rest, HRV_rest, M_rest)\n",
        "df_w = simulate_outside_factors(df_w)\n",
        "\n",
        "# 5. Final Split\n",
        "feature_cols = ['hr', 'hr_diff', 'movement', 'Sleep', 'Screen', 'OutsideFactors']\n",
        "target_col = 'AttentionScore'\n",
        "clean_df = df_w.dropna(subset=feature_cols + [target_col])\n",
        "\n",
        "print(f\"Rows available for training: {len(clean_df)}\")\n",
        "\n",
        "if len(clean_df) > 10:\n",
        "    train_df, test_df = train_test_split(clean_df, test_size=0.25, random_state=42)\n",
        "    \n",
        "    train_path = os.path.join(OUTPUT_DIR, 'train.csv')\n",
        "    test_path = os.path.join(OUTPUT_DIR, 'test.csv')\n",
        "    \n",
        "    train_df.to_csv(train_path, index=False)\n",
        "    test_df.to_csv(test_path, index=False)\n",
        "    \n",
        "    print(\"\\nüéâ SUCCESS! Data is split and saved.\")\n",
        "    print(f\"Train path: {train_path}\")\n",
        "    print(f\"Test path: {test_path}\")\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è Still not enough data. Check the timestamp range printout above.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SUCCESS!\n",
            "Train set: (7120, 12) saved to ./attention_output/train.csv\n",
            "Test set:  (2374, 12) saved to ./attention_output/test.csv\n"
          ]
        }
      ],
      "source": [
        "# ---- TRAIN/TEST SPLIT ----\n",
        "\n",
        "# Define the features we want to use for training\n",
        "feature_cols = ['hr', 'hr_diff', 'movement', 'Sleep', 'Screen', 'OutsideFactors']\n",
        "target_col = 'AttentionScore'\n",
        "\n",
        "# Clean out any rows that somehow got NaNs (rare but possible)\n",
        "clean_df = df_w.dropna(subset=feature_cols + [target_col])\n",
        "\n",
        "# Split: 75% Train, 25% Test\n",
        "train_df, test_df = train_test_split(clean_df, test_size=0.25, random_state=42)\n",
        "\n",
        "# Save the final files\n",
        "train_path = os.path.join(OUTPUT_DIR, 'train.csv')\n",
        "test_path = os.path.join(OUTPUT_DIR, 'test.csv')\n",
        "\n",
        "train_df.to_csv(train_path, index=False)\n",
        "test_df.to_csv(test_path, index=False)\n",
        "\n",
        "print(f\"SUCCESS!\")\n",
        "print(f\"Train set: {train_df.shape} saved to {train_path}\")\n",
        "print(f\"Test set:  {test_df.shape} saved to {test_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6183f79",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9324f132",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
